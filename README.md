# ğŸ“ Travel Planner â€“ AI Agent System

This project is a multi-agent Travel Planner powered by the **A2A (Agent-to-Agent)** protocol. It uses LangChain, Ollama LLM, and multiple A2A-compatible agents to collaboratively fetch weather data, search results, and generate a final travel recommendation.

---

### ğŸš€ Features

* ğŸŒ¦ï¸ **WeatherAgent** â€“ Provides weather info for the destination.
* ğŸ” **TavilySearchAgent** â€“ Searches the web for relevant information.
* ğŸ¤– **LocalLLM** â€“ Hosts a local LLM (`llama3.2`) using `langchain_ollama`.
* ğŸ§  **TravelPlanner** â€“ Coordinates the above agents using the A2A protocol.

---

### ğŸ§© Architecture

```mermaid
graph TD;
    User --> TravelPlanner
    TravelPlanner --> WeatherAgent
    TravelPlanner --> TavilySearchAgent
    TravelPlanner --> LocalLLM
```

---

### ğŸ“ Directory Structure

```
travel-planner/
â”‚
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ WeatherAgent.py             # Weather API Agent (A2A)
â”œâ”€â”€ TavilySearchAgent.py        # Search Agent using Tavily (A2A)
â”œâ”€â”€ LocalLLM.py                 # Local LLM server using Ollama + LangChain
â”œâ”€â”€ Travel_Planner_Agent.py     # Main Planner Agent
â”œâ”€â”€ TravelPlannerApp.py         # Main Streamlit  App
â”œâ”€â”€ main.py                     # Runs agents sequentially with delay
â””â”€â”€ README.md                   # This file
```

---

### âš™ï¸ Setup

#### 1. ğŸ“¦ Install Dependencies

```bash
uv venv
.venv\Scripts\activate
uv sync
```

#### 2. ğŸ§ª Configure `.env`

Ensure you have the following keys in your `.env` file:

```dotenv
OPENWEATHER_API_KEY=your_weather_key
TAVILY_API_KEY=your_tavily_key
```

#### 3. ğŸƒâ€â™‚ï¸ Run All Agents

```bash
uv run main.py
```

This script launches all agents one by one with a 2-second delay.

---

### ğŸ›  Tech Stack

* ğŸ§  `LangChain` + `Ollama`
* ğŸ”„ `python_a2a` (Agent-to-Agent protocol)
* ğŸŒ `FastAPI` under the hood
* â›… Weather & Search APIs

---

### ğŸ“ Notes

* All agents expose A2A-compatible APIs.
* The servers **must run concurrently** for A2A to function properly.
* Best used with `llama3.2` loaded in your Ollama setup.